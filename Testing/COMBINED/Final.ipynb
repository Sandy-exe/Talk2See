{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 23:20:53.145944: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-14 23:20:53.202379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-14 23:20:54.409555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#import stuffs\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import requests\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "from ultralytics import YOLO  # Ensure YOLO model is imported\n",
    "import pyttsx3  # For text-to-speech\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yolo and define constants\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolov8m.pt\")  # Adjust to your model's path\n",
    "object_real_width = 20.0  # Real width of the object in cm\n",
    "focal_length = 840        # Focal length of the camera (example value)\n",
    "\n",
    "# API Token and URL for image captioning\n",
    "API_TOKEN = \"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\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"sk-proj-cXLhIgKLVWjY05X0EdPVT3BlbkFJcpOLCPa72OUdnpLOyI5r\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OCR(image):\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"Error\"\n",
    "\n",
    "def query_image_captioning(image_path):\n",
    "    API_TOKEN = \"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\"\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-large\"\n",
    "    headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "    \n",
    "    with open(image_path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, data=data)\n",
    "    return response.json()\n",
    "\n",
    "def is_blurry(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    fm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return fm < 50, fm  # Adjust the threshold as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_api(input_text):\n",
    "    print(\"input_text:\", input_text)\n",
    "    messages = [\n",
    "        \n",
    "        {\"role\": \"system\", \"content\": \"Process the given input by correcting and enhancing the information in the 'Image Captioning' section. If the text cannot be corrected or is unreadable, return 'Error'. Additionally, interpret the information provided in the 'OCR' and 'Object Distance' sections. Provide clear, concise responses that are accessible to a visually impaired individual. Use simple language to describe the scene, text, and distance details, ensuring the response is easy to understand.\"},\n",
    "    ]\n",
    "\n",
    "    if input_text:\n",
    "        messages.append({\"role\": \"user\", \"content\": input_text})\n",
    "\n",
    "    try:\n",
    "        chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)\n",
    "        reply = chat_completion.choices[0].message['content']\n",
    "        print(chat_completion.choices[0].message)\n",
    "        return reply\n",
    "    except Exception as e:\n",
    "        print(\"Error during OpenAI API call:\", e)\n",
    "        return \"Error\"\n",
    "\n",
    "def speak(text):\n",
    "    try: \n",
    "        if text == \"Error\":\n",
    "            engine.say(\"Sorry, I am unable to process the information at the moment. Please try again later.\")\n",
    "            engine.runAndWait()\n",
    "            return\n",
    "        \n",
    "        print(\"Speaking..\")\n",
    "        engine.say(text)\n",
    "        \n",
    "        engine.runAndWait()\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_object_detection(image,model):\n",
    "    try:\n",
    "        results = model.predict(image)\n",
    "        detections = results[0].boxes\n",
    "        DETECTIONS = ''\n",
    "        for box in detections:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            class_id = int(box.cls[0].item())\n",
    "            prob = round(box.conf[0].item(), 2)\n",
    "            label = model.names[class_id]\n",
    "            object_width_pixels = x2 - x1\n",
    "            object_real_width = 20.0\n",
    "            focal_length = 840\n",
    "\n",
    "            if object_width_pixels > 0 and prob > 0.5:\n",
    "                distance = (object_real_width * focal_length) / object_width_pixels\n",
    "                DETECTIONS += f'Distance of {label}: {distance:.2f} cm\\n'\n",
    "        return DETECTIONS\n",
    "    except Exception as e:\n",
    "        print(f\"Error in YOLO detection: {e}\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_with_threads(frame):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        global speak_thread_running\n",
    "\n",
    "        # Shared variables to hold the results from each thread\n",
    "        yolo_result = [None]\n",
    "        ocr_result = [None]\n",
    "        caption_result = [None]\n",
    "\n",
    "        # YOLO detection thread\n",
    "        def yolo_thread():\n",
    "            yolo_result[0] = yolo_object_detection(frame, model)\n",
    "\n",
    "        # OCR thread\n",
    "        def ocr_thread():\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            ocr_result[0] = get_OCR(gray)\n",
    "\n",
    "        # Image Captioning thread\n",
    "        def caption_thread():\n",
    "            cv2.imwrite('temp_image.jpg', frame)\n",
    "            caption_output = query_image_captioning('temp_image.jpg')\n",
    "            if isinstance(caption_output, list) and 'generated_text' in caption_output[0]:\n",
    "                caption_result[0] = caption_output[0]['generated_text']\n",
    "            else:\n",
    "                caption_result[0] = \"Error\"\n",
    "\n",
    "        # Create and start the threads\n",
    "        threads = []\n",
    "        threads.append(threading.Thread(target=yolo_thread))\n",
    "        threads.append(threading.Thread(target=ocr_thread))\n",
    "        threads.append(threading.Thread(target=caption_thread))\n",
    "\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        # Check if all threads resulted in errors\n",
    "        if yolo_result[0] == \"Error\" and ocr_result[0] == \"Error\" and caption_result[0] == \"Error\":\n",
    "            chatgpt_response = \"Threads not working\"\n",
    "        else:\n",
    "            # Combine results\n",
    "            combined_text = f\"YOLO Detection: {yolo_result[0]}\\nOCR: {ocr_result[0]}\\nCaption: {caption_result[0]}\"\n",
    "\n",
    "            # Send to ChatGPT for final processing\n",
    "            chatgpt_response = chatgpt_api(combined_text)\n",
    "        \n",
    "        # Speak the final response\n",
    "        if not speak_thread_running:\n",
    "            print(\"Starting speak thread\")\n",
    "            speak_thread = threading.Thread(target=speak, args=(chatgpt_response,))\n",
    "            speak_thread.start()\n",
    "            speak_thread_running = True\n",
    "            \n",
    "        print(\"chatgpt_response:\", chatgpt_response)\n",
    "        return chatgpt_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_frame_with_threads: {e}\")\n",
    "        return \"Error\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread']\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# Example thread names to exclude\n",
    "exclude_thread_names = ['process_frame_thread', 'yolo_thread']\n",
    "def stop_threads():\n",
    "    global stop_event\n",
    "    stop_event.set()\n",
    "# Get the list of active threads\n",
    "active_threads = threading.enumerate()\n",
    "\n",
    "# Filter out specific threads\n",
    "filtered_threads = [thread for thread in active_threads if thread.name not in exclude_thread_names]\n",
    "stop_threads()\n",
    "\n",
    "# Print the filtered list of active threads\n",
    "print(f\"Active threads: {[thread.name for thread in filtered_threads]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread']\n",
      "Processing frame...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/santhosh/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-4 (process_frame_thread)', 'Thread-5 (yolo_thread)', 'Thread-7 (caption_thread)']\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-4 (process_frame_thread)', 'Thread-5 (yolo_thread)', 'Thread-7 (caption_thread)', 'Thread-8 (func)']\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-4 (process_frame_thread)', 'Thread-5 (yolo_thread)', 'Thread-7 (caption_thread)']\n",
      "0: 480x640 1 person, 1 bed, 98.6ms\n",
      "Speed: 3.6ms preprocess, 98.6ms inference, 467.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-4 (process_frame_thread)', 'Thread-7 (caption_thread)']\n",
      "input_text: YOLO Detection: Distance of person: 35.07 cm\n",
      "\n",
      "OCR:  \n",
      "\f\n",
      "Caption: there is a man sitting in a room with a remote control\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-4 (process_frame_thread)']\n",
      "Error during OpenAI API call: Incorrect API key provided: sk-proj-********************************************yI5r. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "Starting speak thread\n",
      "chatgpt_response: Error\n",
      "Response: Error\n",
      "Processing frame...\n",
      "\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-9 (speak)', 'Thread-10 (process_frame_thread)', 'Thread-11 (yolo_thread)', 'Thread-12 (ocr_thread)', 'Thread-13 (caption_thread)']\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-10 (process_frame_thread)', 'Thread-11 (yolo_thread)', 'Thread-12 (ocr_thread)', 'Thread-13 (caption_thread)']\n",
      "0: 480x640 2 persons, 1 bed, 149.9ms\n",
      "Speed: 6.4ms preprocess, 149.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-10 (process_frame_thread)', 'Thread-12 (ocr_thread)', 'Thread-13 (caption_thread)']\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-10 (process_frame_thread)', 'Thread-13 (caption_thread)']\n",
      "input_text: YOLO Detection: Distance of person: 34.78 cm\n",
      "\n",
      "OCR:  \n",
      "\f\n",
      "Caption: there is a man that is looking at a cell phone\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-10 (process_frame_thread)']\n",
      "Error during OpenAI API call: Incorrect API key provided: sk-proj-********************************************yI5r. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "chatgpt_response: Error\n",
      "Response: Error\n",
      "Processing frame...\n",
      "\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-14 (process_frame_thread)', 'Thread-15 (yolo_thread)', 'Thread-16 (ocr_thread)', 'Thread-17 (caption_thread)']\n",
      "0: 480x640 2 persons, 1 couch, 84.8ms\n",
      "Speed: 5.4ms preprocess, 84.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-14 (process_frame_thread)', 'Thread-16 (ocr_thread)', 'Thread-17 (caption_thread)']\n",
      "Active threads: ['MainThread', 'IOPub', 'Heartbeat', 'Thread-1 (_watch_pipe_fd)', 'Thread-2 (_watch_pipe_fd)', 'Control', 'IPythonHistorySavingThread', 'Thread-14 (process_frame_thread)', 'Thread-17 (caption_thread)']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text: YOLO Detection: Distance of person: 33.14 cm\n",
      "\n",
      "OCR:  \n",
      "\f\n",
      "Caption: there is a man sitting in a room with a remote control\n",
      "Error during OpenAI API call: Incorrect API key provided: sk-proj-********************************************yI5r. You can find your API key at https://platform.openai.com/account/api-keys.\n",
      "chatgpt_response: Error\n",
      "Response: Error\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    assert cap.isOpened(), \"Error opening camera\"\n",
    "\n",
    "    global speak_thread_running\n",
    "    speak_thread_running = False\n",
    "    \n",
    "    prev_active_threads = None\n",
    "\n",
    "\n",
    "    global image_processing\n",
    "    image_processing = False\n",
    "\n",
    "    def process_frame_thread(frame):\n",
    "        response = process_frame_with_threads(frame)\n",
    "        global image_processing\n",
    "        image_processing = False\n",
    "        print(\"Response:\", response)\n",
    "        \n",
    "\n",
    "    while cap.isOpened():\n",
    "        active_threads = threading.enumerate()\n",
    "        if active_threads != prev_active_threads:\n",
    "            print(f\"Active threads: {[thread.name for thread in active_threads]}\")\n",
    "            prev_active_threads = active_threads\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            break\n",
    "\n",
    "        # Process frame using threads for YOLO, OCR, and Image Captioning\n",
    "        if not is_blurry(frame)[0] and not image_processing:\n",
    "            print(\"Processing frame...\")\n",
    "            image_processing = True\n",
    "            thread = threading.Thread(target=process_frame_thread, args=(frame,))\n",
    "            thread.start()\n",
    "        \n",
    "        # Display the frame\n",
    "        if image_processing:\n",
    "            cv2.putText(frame, \"Processing...\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif is_blurry(frame):\n",
    "            cv2.putText(frame, \"Image is Blurry\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Press 'q' to exit\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "        cv2.imshow('Camera Feed', frame)\n",
    "        \n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
